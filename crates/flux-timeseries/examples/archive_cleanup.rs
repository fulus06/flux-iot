use chrono::Duration;
use flux_timeseries::{
    ArchiveDestination, ArchivePolicy, CleanupManager, CleanupPolicy, DataArchiver,
    DownsampleManager, DownsamplePolicy, TimescaleStore,
};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // åˆå§‹åŒ–æ—¥å¿—
    tracing_subscriber::fmt::init();

    println!("ğŸ—„ï¸  FLUX TimeSeries Archive & Cleanup Example\n");

    // è¿æ¥æ•°æ®åº“
    let database_url = "postgresql://postgres:postgres@localhost:5432/flux_iot";
    let store = TimescaleStore::new(database_url).await?;
    let db = store.connection();
    println!("âœ… Connected to TimescaleDB\n");

    // 1. æ•°æ®é™é‡‡æ ·
    println!("ğŸ“Š Creating downsample views...");
    let downsample_mgr = DownsampleManager::new(db.clone().into());

    // åˆ›å»ºæ—¥èšåˆè§†å›¾
    let daily_policy = DownsamplePolicy::daily();
    downsample_mgr.create_downsample_view(&daily_policy).await?;
    downsample_mgr.add_refresh_policy(&daily_policy).await?;
    downsample_mgr.add_retention_policy(&daily_policy).await?;
    println!("  âœ“ Daily downsample view created");

    // åˆ›å»ºå‘¨èšåˆè§†å›¾
    let weekly_policy = DownsamplePolicy::weekly();
    downsample_mgr.create_downsample_view(&weekly_policy).await?;
    downsample_mgr.add_refresh_policy(&weekly_policy).await?;
    println!("  âœ“ Weekly downsample view created");

    // åˆ—å‡ºæ‰€æœ‰é™é‡‡æ ·è§†å›¾
    let views = downsample_mgr.list_views().await?;
    println!("  âœ“ Total downsample views: {}", views.len());
    for view in &views {
        println!("    - {}", view);
    }
    println!();

    // 2. å­˜å‚¨ç»Ÿè®¡
    println!("ğŸ“ˆ Getting storage statistics...");
    let cleanup_mgr = CleanupManager::new(db.clone().into());
    let stats = cleanup_mgr.get_storage_stats().await?;
    
    println!("  âœ“ Total size: {:.2} MB", stats.total_size_mb);
    println!("  âœ“ Compressed size: {:.2} MB", stats.compressed_size_mb);
    println!("  âœ“ Compression ratio: {:.2}x", stats.compression_ratio);
    println!("  âœ“ Chunk count: {}", stats.chunk_count);
    println!("  Table sizes:");
    for (table, size) in &stats.table_sizes {
        println!("    - {}: {:.2} MB", table, size);
    }
    println!();

    // 3. æ•°æ®æ¸…ç†
    println!("ğŸ§¹ Running cleanup tasks...");
    
    // æ¸…ç†æŒ‡æ ‡æ•°æ®ï¼ˆä¿ç•™90å¤©ï¼‰
    let metrics_policy = CleanupPolicy::for_metrics();
    let cleanup_stats = cleanup_mgr.cleanup(&metrics_policy).await?;
    println!(
        "  âœ“ Metrics cleanup: {} rows deleted, {:.2} MB freed",
        cleanup_stats.deleted_rows, cleanup_stats.freed_space_mb
    );

    // æ¸…ç†æ—¥å¿—æ•°æ®ï¼ˆä¿ç•™30å¤©ï¼‰
    let logs_policy = CleanupPolicy::for_logs();
    let cleanup_stats = cleanup_mgr.cleanup(&logs_policy).await?;
    println!(
        "  âœ“ Logs cleanup: {} rows deleted, {:.2} MB freed",
        cleanup_stats.deleted_rows, cleanup_stats.freed_space_mb
    );
    println!();

    // 4. æ•°æ®å½’æ¡£
    println!("ğŸ“¦ Running archive task...");
    let archiver = DataArchiver::new(db.clone().into());
    
    let archive_policy = ArchivePolicy {
        table_name: "device_metrics".to_string(),
        archive_older_than: Duration::days(365), // å½’æ¡£1å¹´å‰çš„æ•°æ®
        destination: ArchiveDestination::LocalFile {
            path: "/tmp/flux_iot_archive.json".to_string(),
        },
        delete_after_archive: false, // ä¸åˆ é™¤åŸå§‹æ•°æ®
    };

    let archive_stats = archiver.archive(&archive_policy).await?;
    println!(
        "  âœ“ Archived {} rows ({:.2} MB) to {}",
        archive_stats.archived_rows,
        archive_stats.archive_size_mb,
        archive_stats.destination
    );
    println!();

    println!("âœ¨ Archive & Cleanup example completed successfully!");

    Ok(())
}
